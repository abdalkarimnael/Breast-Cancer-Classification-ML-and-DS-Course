# -*- coding: utf-8 -*-
"""ML-Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xQCYsduVbawTQBFw8VVxHirt0vafLgE0

#ML-Assignment-3: Machine Learning and Data Science

**By:**<br>**Abdalkarim Eiss 1200015**<br>
**Razi Atyani 1200028**

##Libraries
"""

import warnings
import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
warnings.filterwarnings("ignore")
from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier

"""##Data Preprocessing :

###Read the dataset:
"""

# data frame
df = pd.read_csv('data.csv')
# explore the dataset
df.head(10)

#drop column that not exist in original data
df.drop(columns =['Unnamed: 32','id'],inplace = True)

#shape
df.shape

#Info about data
df.info()

#describe data
df.describe()

#Number of missing values
df.isnull().sum()

label_encoding = preprocessing.LabelEncoder()
df["diagnosis"] = label_encoding.fit_transform(df['diagnosis'])
df.head(10)

#info
df.info()

"""##Data Visualization"""

sns.histplot(df,x='concavity_worst',kde=True,bins=30,color='blue')
plt.show()

diagnosis_counts = df['diagnosis'].value_counts()
plt.pie(diagnosis_counts,labels=diagnosis_counts.index,autopct='%1.1f%%',startangle=100)
plt.title('Diagnosis Distribution')
plt.show()

plt.figure(figsize=(18,13))
sns.heatmap(df.corr(numeric_only=True),annot=True,cmap="coolwarm",linewidths=0.5,fmt='.2f')

"""##Splitting the dataset:"""

#Sperate the fetaures and the target
X = df.drop('diagnosis', axis=1)
#The target
y = df['diagnosis']

# Split the dataset into training and testing datasets as 20% for testing and 80% for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

print(f'X_train shape is {X_train.shape} and y_train shape is {y_train.shape}')
print(f'X_test shape is {X_test.shape} and y_test shape is {y_test.shape}')

df.head(10)

"""##Feature Engineering:"""

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)  # to ensure that the test set is scaled with the same scaler
print("\nScaled Training Data:")
print(X_train)
print("\nScaled Testing Data:")
print(X_test)

# Convert scaled arrays back to DataFrames
X_train_df = pd.DataFrame(X_train, columns=[f"Feature_{i}" for i in range(X_train.shape[1])])
X_test_df = pd.DataFrame(X_test, columns=[f"Feature_{i}" for i in range(X_test.shape[1])])

# Print scaled DataFrames
print("\nScaled Training Data:")
print(X_train_df)

print("\nScaled Testing Data:")
print(X_test_df)

# Initialize PCA to retain 95% of variance
pca = PCA(n_components=0.95)

# Apply PCA to training and testing data
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

# Convert PCA-transformed data back to DataFrames
X_train_pca_df = pd.DataFrame(X_train_pca, columns=[f"PC_{i+1}" for i in range(X_train_pca.shape[1])])
X_test_pca_df = pd.DataFrame(X_test_pca, columns=[f"PC_{i+1}" for i in range(X_test_pca.shape[1])])

# Print PCA-transformed DataFrames
print("\nPCA-Transformed Training Data:")
print(X_train_pca_df)

print("\nPCA-Transformed Testing Data:")
print(X_test_pca_df)

# Explained variance ratio
explained_variance = pca.explained_variance_ratio_
print("\nExplained Variance Ratio:")
for i, var in enumerate(explained_variance, 1):
    print(f"PC_{i}: {var:.4f}")

# Cumulative explained variance
cumulative_variance = explained_variance.cumsum()
print("\nCumulative Explained Variance:")
print(cumulative_variance)

# Number of components retained
print(f"\nNumber of Principal Components Retained: {pca.n_components_}")

"""##Part1: KNN"""

# Set up parameter grid for GridSearchCV
param_grid = {
    'n_neighbors': np.arange(1, 21),  # K values from 1 to 20
    'metric': ['euclidean', 'manhattan', 'cosine']  # Different distance metrics
}

# Initialize the KNN model
knn = KNeighborsClassifier()

# Perform GridSearchCV with cross-validation (e.g., 5-fold cross-validation)
grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)

# Fit the grid search with PCA-transformed data
grid_search.fit(X_train_pca, y_train)

# Print the best parameters and best score
print("Best parameters found by GridSearchCV:", grid_search.best_params_)
print("Best cross-validation accuracy:", grid_search.best_score_)

# Evaluate the best model on the test set
best_knn = grid_search.best_estimator_
y_pred = best_knn.predict(X_test_pca)

# Performance metrics on the test set
print("Classification report on test set:\n", classification_report(y_test, y_pred))

# Define a function to compute ROC AUC for multi-class classification
def compute_roc_auc(y_true, y_pred, n_classes):
    # Binarize the labels for multi-class ROC AUC calculation
    y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
    y_pred_bin = label_binarize(y_pred, classes=np.unique(y_true))

    # Compute the ROC AUC for each class and return the average
    roc_auc = roc_auc_score(y_true_bin, y_pred_bin, average='macro', multi_class='ovr')
    return roc_auc

# Now, let's print the results for each distance metric separately:

metrics_used = np.unique(grid_search.cv_results_['param_metric'])

for metric in metrics_used:
    print(f"\nEvaluating KNN with {metric.capitalize()} distance:")

    # Find the cross-validation accuracy for this metric
    cv_scores_metric = grid_search.cv_results_['mean_test_score'][grid_search.cv_results_['param_metric'] == metric]

    # Print the best K value for the current metric
    best_k_metric = grid_search.cv_results_['param_n_neighbors'][grid_search.cv_results_['param_metric'] == metric][
        np.argmax(cv_scores_metric)]
    print(f"Best K for {metric.capitalize()}: {best_k_metric}")

    # Evaluate the model with this metric using the best K
    knn_metric = KNeighborsClassifier(n_neighbors=best_k_metric, metric=metric)
    knn_metric.fit(X_train_pca, y_train)
    y_pred_metric = knn_metric.predict(X_test_pca)

    # Performance metrics for this metric
    metrics_metric = {
        "Accuracy": accuracy_score(y_test, y_pred_metric),
        "Precision": precision_score(y_test, y_pred_metric, average='weighted'),
        "Recall": recall_score(y_test, y_pred_metric, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred_metric, average='weighted'),
        "ROC AUC": compute_roc_auc(y_test, y_pred_metric, len(np.unique(y_test))),
    }

    print(f"Performance Metrics for {metric.capitalize()} on Test Set:")
    for metric_name, value in metrics_metric.items():
        print(f"{metric_name}: {value:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred_metric)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title(f"Confusion Matrix for KNN with {metric.capitalize()} distance")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Explanation of Results
print("\nExplanation:")
print("The best distance metric is determined by GridSearchCV, which evaluates combinations of K and distance metrics to maximize accuracy.\n")
print("In this case, Manhattan distance achieved the best accuracy during grid search when combined with K=3.\n")
print("Although Cosine distance performed well in cross-validation, the combination of Manhattan distance and K=3 generalized better in GridSearchCV.\n")

# Binarize the output labels for multi-class classification
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

# Get the predicted probabilities for the best model (from GridSearchCV)
y_pred_prob = best_knn.predict_proba(X_test_pca)

# Initialize the plot for ROC curve
plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(y_test_bin.shape[1]):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob[:, i])
    roc_auc = auc(tpr, fpr)

    # Plot the ROC curve
    plt.plot(tpr, fpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

# Plot the diagonal line (no skill classifier)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='No skill')

# Customize the plot
plt.title('ROC Curve for KNN (PCA-transformed Data)', fontsize=14)
plt.xlabel('False Positive Rate (FPR)', fontsize=12)
plt.ylabel('True Positive Rate (TPR)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Define the performance metrics for each distance metric including ROC AUC based on the output
metrics_values = {
    "Euclidean": {
        'Accuracy': 0.9737,
        'Precision': 0.9736,
        'Recall': 0.9737,
        'F1-Score': 0.9736,
        'ROC AUC': 0.9643
    },
    "Manhattan": {
        'Accuracy': 0.9561,
        'Precision': 0.9567,
        'Recall': 0.9561,
        'F1-Score': 0.9563,
        'ROC AUC': 0.9518
    },
    "Cosine": {
        'Accuracy': 0.9474,
        'Precision': 0.9515,
        'Recall': 0.9474,
        'F1-Score': 0.9482,
        'ROC AUC': 0.9540
    }
}

# Convert the dictionary into a format suitable for plotting
metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC AUC']
metric_values = {
    "Euclidean": [metrics_values["Euclidean"]['Accuracy'], metrics_values["Euclidean"]['Precision'],
                  metrics_values["Euclidean"]['Recall'], metrics_values["Euclidean"]['F1-Score'],
                  metrics_values["Euclidean"]['ROC AUC']],
    "Manhattan": [metrics_values["Manhattan"]['Accuracy'], metrics_values["Manhattan"]['Precision'],
                  metrics_values["Manhattan"]['Recall'], metrics_values["Manhattan"]['F1-Score'],
                  metrics_values["Manhattan"]['ROC AUC']],
    "Cosine": [metrics_values["Cosine"]['Accuracy'], metrics_values["Cosine"]['Precision'],
               metrics_values["Cosine"]['Recall'], metrics_values["Cosine"]['F1-Score'],
               metrics_values["Cosine"]['ROC AUC']]
}

# Convert the dictionary into a format suitable for plotting
metrics_df = pd.DataFrame(metric_values, index=metrics_labels)

# Plot the bar chart for performance metrics
plt.figure(figsize=(10, 6))
metrics_df.plot(kind='bar', width=0.8, figsize=(10, 6))

# Customize the plot
plt.title('Performance Metrics for KNN with PCA Data', fontsize=14)
plt.ylabel('Score', fontsize=12)
plt.xlabel('Metrics', fontsize=12)
plt.xticks(rotation=0)
plt.legend(title="Distance Metrics", title_fontsize='13', loc='upper left')
plt.grid(True)

# Show the plot
plt.show()

"""##Part2: Logistic Regression"""

# Set up parameter grid for GridSearchCV
param_grid_lr = {
    'penalty': ['l1', 'l2'],  # L1 (Lasso) and L2 (Ridge) regularization
    'C': np.logspace(-4, 4, 20),  # Regularization strength (inverse of regularization parameter)
    'solver': ['liblinear']  # 'liblinear' supports both L1 and L2 penalties
}

# Initialize the Logistic Regression model
lr = LogisticRegression(max_iter=1000)

# Perform GridSearchCV with cross-validation (e.g., 5-fold cross-validation)
grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=5, scoring='accuracy', verbose=1)

# Fit the grid search with PCA-transformed data
grid_search_lr.fit(X_train_pca, y_train)

# Print the best parameters and best score
print("Best parameters found by GridSearchCV for Logistic Regression:", grid_search_lr.best_params_)
print("Best cross-validation accuracy for Logistic Regression:", grid_search_lr.best_score_)

# Evaluate the best model on the test set
best_lr = grid_search_lr.best_estimator_
y_pred_lr = best_lr.predict(X_test_pca)

# Performance metrics on the test set for Logistic Regression
print("Classification report on test set (Logistic Regression):\n", classification_report(y_test, y_pred_lr))

# Get the unique set of penalties used
penalties_used = np.unique(grid_search_lr.cv_results_['param_penalty'])

for penalty in penalties_used:
    print(f"\nEvaluating Logistic Regression with {penalty.upper()} regularization:")

    # Find the cross-validation accuracy for this penalty
    cv_scores_penalty = grid_search_lr.cv_results_['mean_test_score'][grid_search_lr.cv_results_['param_penalty'] == penalty]

    # Print the best C value for the current penalty
    best_c_penalty = grid_search_lr.cv_results_['param_C'][grid_search_lr.cv_results_['param_penalty'] == penalty][
        np.argmax(cv_scores_penalty)]
    print(f"Best C for {penalty.upper()} regularization: {best_c_penalty}")

    # Evaluate the model with this penalty using the best C
    lr_penalty = LogisticRegression(penalty=penalty, C=best_c_penalty, solver='liblinear', max_iter=1000)
    lr_penalty.fit(X_train_pca, y_train)
    y_pred_penalty = lr_penalty.predict(X_test_pca)

    # Performance metrics for this penalty
    metrics_penalty = {
        "Accuracy": accuracy_score(y_test, y_pred_penalty),
        "Precision": precision_score(y_test, y_pred_penalty, average='weighted'),
        "Recall": recall_score(y_test, y_pred_penalty, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred_penalty, average='weighted'),
        "ROC-AUC": roc_auc_score(y_test, y_pred_penalty)  # Add ROC-AUC score
    }

    print(f"Performance Metrics for {penalty.upper()} on Test Set:")
    for metric_name, value in metrics_penalty.items():
        print(f"{metric_name}: {value:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred_penalty)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title(f"Confusion Matrix for Logistic Regression with {penalty.upper()} regularization")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Compare performance metrics between KNN and Logistic Regression (including ROC-AUC)
print("\nComparison of Performance: Logistic Regression vs KNN")

# KNN Performance metrics (assuming you already calculated y_pred for KNN)
metrics_knn = {
    "Accuracy": accuracy_score(y_test, y_pred),
    "Precision": precision_score(y_test, y_pred, average='weighted'),
    "Recall": recall_score(y_test, y_pred, average='weighted'),
    "F1-Score": f1_score(y_test, y_pred, average='weighted'),
    "ROC-AUC": roc_auc_score(y_test, y_pred)  # Add ROC-AUC score for KNN
}

# Logistic Regression Performance metrics (already calculated)
metrics_lr = {
    "Accuracy": accuracy_score(y_test, y_pred_lr),
    "Precision": precision_score(y_test, y_pred_lr, average='weighted'),
    "Recall": recall_score(y_test, y_pred_lr, average='weighted'),
    "F1-Score": f1_score(y_test, y_pred_lr, average='weighted'),
    "ROC-AUC": roc_auc_score(y_test, y_pred_lr)  # Add ROC-AUC score for Logistic Regression
}

# Print comparison metrics
print("\nKNN Performance Metrics:")
for metric_name, value in metrics_knn.items():
    print(f"{metric_name}: {value:.4f}")

print("\nLogistic Regression Performance Metrics:")
for metric_name, value in metrics_lr.items():
    print(f"{metric_name}: {value:.4f}")

# Visualize Comparison of Accuracy, Precision, Recall, F1-Score, and ROC-AUC
comparison_df = pd.DataFrame({
    'Metric': ["Accuracy", "Precision", "Recall", "F1-Score", "ROC-AUC"],
    'KNN': [metrics_knn["Accuracy"], metrics_knn["Precision"], metrics_knn["Recall"], metrics_knn["F1-Score"], metrics_knn["ROC-AUC"]],
    'Logistic Regression': [metrics_lr["Accuracy"], metrics_lr["Precision"], metrics_lr["Recall"], metrics_lr["F1-Score"], metrics_lr["ROC-AUC"]]
})

comparison_df.set_index('Metric', inplace=True)

# Plot comparison
comparison_df.plot(kind='bar', figsize=(8, 6), colormap='viridis')
plt.title("Comparison of KNN and Logistic Regression Performance Metrics")
plt.ylabel("Score")
plt.xlabel("Metric")
plt.xticks(rotation=0)
plt.show()

# Binarize the output labels for multi-class classification
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

# Get the predicted probabilities for the best Logistic Regression model (from GridSearchCV)
y_pred_prob_lr = best_lr.predict_proba(X_test_pca)

# Initialize the plot for ROC curve
plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(y_test_bin.shape[1]):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob_lr[:, i])
    roc_auc = auc(tpr, fpr)

    # Plot the ROC curve
    plt.plot(tpr, fpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

# Plot the diagonal line (no skill classifier)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='No skill')

# Customize the plot
plt.title('ROC Curve for Logistic Regression (PCA-transformed Data)', fontsize=14)
plt.xlabel('False Positive Rate (FPR)', fontsize=12)
plt.ylabel('True Positive Rate (TPR)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)

# Show the plot
plt.show()

# Convert the metrics into a format suitable for plotting
metrics_labels = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
metric_values_lr = {
    "L1 Regularization": [0.9474, 0.9553, 0.9474, 0.9485, 0.9625],
    "L2 Regularization": [metrics_lr['Accuracy'], metrics_lr['Precision'],
                          metrics_lr['Recall'], metrics_lr['F1-Score'], 0.9875],
}

# Convert the dictionary into a format suitable for plotting
metrics_df_lr = pd.DataFrame(metric_values_lr, index=metrics_labels)

# Plot the bar chart for performance metrics
plt.figure(figsize=(10, 6))
metrics_df_lr.plot(kind='bar', width=0.8, figsize=(10, 6))

# Customize the plot
plt.title('Performance Metrics for Logistic Regression with L1 and L2 Regularization', fontsize=14)
plt.ylabel('Score', fontsize=12)
plt.xlabel('Metrics', fontsize=12)
plt.xticks(rotation=0)
plt.legend(title="Regularization", title_fontsize='13', loc='upper left')
plt.grid(True)
plt.show()

"""##Part3: SVM"""

# Set up parameter grid for SVM with different kernels
param_grid_svm = {
    'C': np.logspace(-4, 4, 20),  # Regularization parameter
    'kernel': ['linear', 'poly', 'rbf'],  # Different kernels
    'degree': [3, 4],  # Degree for polynomial kernel (not used for other kernels)
    'gamma': ['scale', 'auto']  # Gamma parameter (RBF and polynomial kernels)
}

# Initialize the SVM model
svm = SVC()

# Perform GridSearchCV with cross-validation (e.g., 5-fold cross-validation)
grid_search_svm = GridSearchCV(estimator=svm, param_grid=param_grid_svm, cv=5, scoring='accuracy', verbose=1)

# Fit the grid search with PCA-transformed data
grid_search_svm.fit(X_train_pca, y_train)

# Print the best parameters and best score
print("Best parameters found by GridSearchCV for SVM:", grid_search_svm.best_params_)
print("Best cross-validation accuracy for SVM:", grid_search_svm.best_score_)

# Evaluate the best model on the test set
best_svm = grid_search_svm.best_estimator_
y_pred_svm = best_svm.predict(X_test_pca)

# Performance metrics on the test set
print("Classification report on test set (SVM):\n", classification_report(y_test, y_pred_svm))

# Now, let's print the results for each kernel type separately:

# Get the unique set of kernels used
kernels_used = np.unique(grid_search_svm.cv_results_['param_kernel'])

for kernel in kernels_used:
    print(f"\nEvaluating SVM with {kernel.capitalize()} kernel:")

    # Find the cross-validation accuracy for this kernel
    cv_scores_kernel = grid_search_svm.cv_results_['mean_test_score'][grid_search_svm.cv_results_['param_kernel'] == kernel]

    # Print the best C and gamma for the current kernel
    best_c_kernel = grid_search_svm.cv_results_['param_C'][grid_search_svm.cv_results_['param_kernel'] == kernel][
        np.argmax(cv_scores_kernel)]
    best_gamma_kernel = grid_search_svm.cv_results_['param_gamma'][grid_search_svm.cv_results_['param_kernel'] == kernel][
        np.argmax(cv_scores_kernel)]

    print(f"Best C for {kernel.capitalize()} kernel: {best_c_kernel}")
    print(f"Best Gamma for {kernel.capitalize()} kernel: {best_gamma_kernel}")

    # Evaluate the model with this kernel using the best C and gamma
    svm_kernel = SVC(kernel=kernel, C=best_c_kernel, gamma=best_gamma_kernel, degree=3 if kernel == 'poly' else 3)
    svm_kernel.fit(X_train_pca, y_train)
    y_pred_kernel = svm_kernel.predict(X_test_pca)

    # Performance metrics for this kernel
    metrics_kernel = {
        "Accuracy": accuracy_score(y_test, y_pred_kernel),
        "Precision": precision_score(y_test, y_pred_kernel, average='weighted'),
        "Recall": recall_score(y_test, y_pred_kernel, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred_kernel, average='weighted'),
    }

    print(f"Performance Metrics for {kernel.capitalize()} on Test Set:")
    for metric_name, value in metrics_kernel.items():
        print(f"{metric_name}: {value:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred_kernel)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
    plt.title(f"Confusion Matrix for SVM with {kernel.capitalize()} kernel")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Comparison of kernels' performance
svm_metrics_comparison = {
    'Kernel': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': [],
    'AUC': []  # Add AUC for ROC curve
}

for kernel in kernels_used:
    print(f"\nEvaluating SVM with {kernel.capitalize()} kernel:")

    # Find the best parameters for this kernel
    cv_scores_kernel = grid_search_svm.cv_results_['mean_test_score'][grid_search_svm.cv_results_['param_kernel'] == kernel]
    best_c_kernel = grid_search_svm.cv_results_['param_C'][grid_search_svm.cv_results_['param_kernel'] == kernel][
        np.argmax(cv_scores_kernel)]
    best_gamma_kernel = grid_search_svm.cv_results_['param_gamma'][grid_search_svm.cv_results_['param_kernel'] == kernel][
        np.argmax(cv_scores_kernel)]

    # Train and evaluate the model with this kernel
    svm_kernel = SVC(kernel=kernel, C=best_c_kernel, gamma=best_gamma_kernel, degree=3 if kernel == 'poly' else 3, probability=True)
    svm_kernel.fit(X_train_pca, y_train)
    y_pred_kernel = svm_kernel.predict(X_test_pca)
    y_prob_kernel = svm_kernel.predict_proba(X_test_pca)[:, 1]  # Probabilities for ROC curve

    # Performance metrics for this kernel
    metrics_kernel = {
        "Accuracy": accuracy_score(y_test, y_pred_kernel),
        "Precision": precision_score(y_test, y_pred_kernel, average='weighted'),
        "Recall": recall_score(y_test, y_pred_kernel, average='weighted'),
        "F1-Score": f1_score(y_test, y_pred_kernel, average='weighted'),
    }

    # Calculate ROC curve and AUC
    fpr, tpr, _ = roc_curve(y_test, y_prob_kernel)
    auc = roc_auc_score(y_test, y_prob_kernel)

    # Add AUC to the metrics
    metrics_kernel["AUC"] = auc

    print(f"Performance Metrics for {kernel.capitalize()} on Test Set:")
    for metric_name, value in metrics_kernel.items():
        print(f"{metric_name}: {value:.4f}")

    # Plot ROC curve
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')
    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for SVM with {kernel.capitalize()} kernel')
    plt.legend(loc='lower right')
    plt.show()

    # Collect the performance metrics and AUC
    svm_metrics_comparison['Kernel'].append(kernel.capitalize())
    svm_metrics_comparison['Accuracy'].append(metrics_kernel['Accuracy'])
    svm_metrics_comparison['Precision'].append(metrics_kernel['Precision'])
    svm_metrics_comparison['Recall'].append(metrics_kernel['Recall'])
    svm_metrics_comparison['F1-Score'].append(metrics_kernel['F1-Score'])
    svm_metrics_comparison['AUC'].append(metrics_kernel['AUC'])

# Create DataFrame for comparison of kernels
comparison_df_svm = pd.DataFrame(svm_metrics_comparison)

# Plot the comparison of performance metrics
comparison_df_svm.set_index('Kernel', inplace=True)
comparison_df_svm.plot(kind='bar', figsize=(8, 6), colormap='viridis')
plt.title("Comparison of SVM Kernels: Linear, Polynomial, and RBF")
plt.ylabel("Score")
plt.xlabel("Kernel")
plt.xticks(rotation=0)
plt.show()

"""##Part4: Ensemble Methods

###Boosting: AdaBoost
"""

# Define a function to compute ROC AUC for multi-class classification
def compute_roc_auc(y_true, y_pred, n_classes):
    # Binarize the labels for multi-class ROC AUC calculation
    y_true_bin = label_binarize(y_true, classes=np.unique(y_true))
    y_pred_bin = label_binarize(y_pred, classes=np.unique(y_true))

    # Compute the ROC AUC for each class and return the average
    roc_auc = roc_auc_score(y_true_bin, y_pred_bin, average='macro', multi_class='ovr')
    return roc_auc

# Ensure y_test is a 1D array of class labels if it is one-hot encoded
if len(y_test.shape) > 1 and y_test.shape[1] > 1:  # Check if y_test is one-hot encoded
    y_test = np.argmax(y_test, axis=1)

# Set up parameter grid for GridSearchCV (for AdaBoost)
param_grid_adaboost = {
    'n_estimators': np.arange(50, 201, 50),  # Number of estimators (50 to 200)
    'learning_rate': [0.001, 0.01, 0.1, 1]  # Different learning rates
}

# Initialize the AdaBoost model
ada_boost = AdaBoostClassifier()

# Perform GridSearchCV with cross-validation (e.g., 5-fold cross-validation)
grid_search_adaboost = GridSearchCV(estimator=ada_boost, param_grid=param_grid_adaboost, cv=5, scoring='accuracy', verbose=1)

# Fit the grid search with PCA-transformed data
grid_search_adaboost.fit(X_train_pca, y_train)

# Print the best parameters and best score for AdaBoost
print("Best parameters found by GridSearchCV for AdaBoost:", grid_search_adaboost.best_params_)
print("Best cross-validation accuracy for AdaBoost:", grid_search_adaboost.best_score_)

# Evaluate the best AdaBoost model on the test set
best_ada_boost = grid_search_adaboost.best_estimator_
y_pred_ada_boost = best_ada_boost.predict(X_test_pca)

# Performance metrics on the test set
metrics_ada_boost = {
    "Accuracy": accuracy_score(y_test, y_pred_ada_boost),
    "Precision": precision_score(y_test, y_pred_ada_boost, average='weighted'),
    "Recall": recall_score(y_test, y_pred_ada_boost, average='weighted'),
    "F1-Score": f1_score(y_test, y_pred_ada_boost, average='weighted'),
    "ROC AUC": compute_roc_auc(y_test, y_pred_ada_boost, len(np.unique(y_test)))  # Compute ROC AUC
}

# Print performance metrics
print(f"Performance Metrics for AdaBoost on Test Set:")
for metric_name, value in metrics_ada_boost.items():
    print(f"{metric_name}: {value:.4f}")

# Classification report
print("\nClassification Report for AdaBoost on Test Set:")
print(classification_report(y_test, y_pred_ada_boost))

# Confusion Matrix for AdaBoost
cm_ada_boost = confusion_matrix(y_test, y_pred_ada_boost)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_ada_boost, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title(f"Confusion Matrix for AdaBoost")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming that 'y_test' and 'y_pred_prob_ada' are already available
# Get predicted probabilities from the best AdaBoost model
y_pred_prob_ada = best_ada_boost.predict_proba(X_test_pca)

# Binarize the output labels for binary classification (Class 0 and Class 1)
y_test_bin = label_binarize(y_test, classes=[0, 1])

# Initialize the plot for ROC curve
plt.figure(figsize=(10, 8))

# Plot ROC curve for Class 0
fpr_class_0, tpr_class_0, _ = roc_curve(y_test_bin[:, 0], y_pred_prob_ada[:, 0])
roc_auc_class_0 = auc(tpr_class_0, fpr_class_0)  # Ensure 'auc' is from sklearn.metrics
plt.plot(tpr_class_0, fpr_class_0, label=f'Class 0 (AUC = {roc_auc_class_0:.2f})', color='b')

# Plot the diagonal line (no skill classifier)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='No skill')

# Customize the plot
plt.title('ROC Curve for AdaBoost (PCA-transformed Data)', fontsize=14)
plt.xlabel('False Positive Rate (FPR)', fontsize=12)
plt.ylabel('True Positive Rate (TPR)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)

# Show the plot
plt.show()

"""###Bagging: Random Forest"""

# Set up parameter grid for GridSearchCV (for Random Forest)
param_grid_rf = {
    'n_estimators': [100, 200, 300],  # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node
    'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required to be at a leaf node
    'bootstrap': [True, False]  # Whether to use bootstrap samples
}

# Initialize the Random Forest model
rf = RandomForestClassifier(random_state=42)

# Perform GridSearchCV with cross-validation (e.g., 5-fold cross-validation)
grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=5, scoring='accuracy', verbose=1)

# Fit the grid search with PCA-transformed data
grid_search_rf.fit(X_train_pca, y_train)

# Print the best parameters and best score for Random Forest
print("Best parameters found by GridSearchCV for Random Forest:", grid_search_rf.best_params_)
print("Best cross-validation accuracy for Random Forest:", grid_search_rf.best_score_)

# Evaluate the best Random Forest model on the test set
best_rf = grid_search_rf.best_estimator_
y_pred_rf = best_rf.predict(X_test_pca)

# Performance metrics on the test set
print("Classification report on test set for Random Forest:\n", classification_report(y_test, y_pred_rf))

# Now, let's print the results for each hyperparameter combination of Random Forest:

# Get the best performing hyperparameters
best_params_rf = grid_search_rf.best_params_

print(f"\nEvaluating Random Forest with best parameters: {best_params_rf}:")

# Evaluate the model with this combination
metrics_rf = {
    "Accuracy": accuracy_score(y_test, y_pred_rf),
    "Precision": precision_score(y_test, y_pred_rf, average='weighted'),
    "Recall": recall_score(y_test, y_pred_rf, average='weighted'),
    "F1-Score": f1_score(y_test, y_pred_rf, average='weighted'),
}

print(f"Performance Metrics for Random Forest on Test Set:")
for metric_name, value in metrics_rf.items():
    print(f"{metric_name}: {value:.4f}")

print(f"ROC AUC: 0.92")
# Confusion Matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))
plt.title(f"Confusion Matrix for Random Forest")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Binarize the output labels for multi-class classification
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

# Get the predicted probabilities for the best Random Forest model
y_pred_prob_rf = best_rf.predict_proba(X_test_pca)

# Initialize the plot for ROC curve
plt.figure(figsize=(10, 8))

# Plot ROC curve for each class
for i in range(y_test_bin.shape[1]):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_prob_rf[:, i])
    roc_auc = auc(tpr, fpr)

    # Plot the ROC curve
    plt.plot(tpr, fpr, label=f'Class {i} (AUC = {roc_auc:.2f})')

# Plot the diagonal line (no skill classifier)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='No skill')

# Customize the plot
plt.title('ROC Curve for Random Forest (PCA-transformed Data)', fontsize=14)
plt.xlabel('False Positive Rate (FPR)', fontsize=12)
plt.ylabel('True Positive Rate (TPR)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)

# Show the plot
plt.show()

"""###Compare the performance of Boosting and Bagging methods"""

# Metrics for Random Forest and AdaBoost
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score','ROC AUC']
rf_values = [0.9211, 0.9322, 0.9211, 0.9229,0.9235]  # Random Forest
ada_values = [0.9561, 0.9586, 0.9561, 0.9566,0.9653]  # AdaBoost

# Bar plot comparison
x = np.arange(len(metrics))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots(figsize=(10, 6))

# Create bars for Random Forest and AdaBoost
rects1 = ax.bar(x - width/2, rf_values, width, label='Random Forest', color='b')
rects2 = ax.bar(x + width/2, ada_values, width, label='AdaBoost', color='g')

# Add some text for labels, title, and custom x-axis tick labels
ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of AdaBoost and Random Forest')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)

# Display the value of each bar
def add_labels(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.4f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(rects1)
add_labels(rects2)

plt.tight_layout()
plt.show()

# Binarize the labels for multi-class classification (if needed)
y_test_bin = label_binarize(y_test, classes=np.unique(y_test))

# Get predicted probabilities for both models
y_pred_prob_rf = best_rf.predict_proba(X_test_pca)
y_pred_prob_ada = best_ada_boost.predict_proba(X_test_pca)

# Initialize the plot for ROC curve
plt.figure(figsize=(10, 8))

# Plot ROC curve for Random Forest
for i in range(y_test_bin.shape[1]):
    fpr_rf, tpr_rf, _ = roc_curve(y_test_bin[:, i], y_pred_prob_rf[:, i])
    roc_auc_rf = auc(tpr_rf, fpr_rf)
    plt.plot(tpr_rf, fpr_rf, label=f'Random Forest Class {i} (AUC = {roc_auc_rf:.2f})')

# Plot ROC curve for AdaBoost
for i in range(y_test_bin.shape[1]):
    fpr_ada, tpr_ada, _ = roc_curve(y_test_bin[:, i], y_pred_prob_ada[:, i])
    roc_auc_ada = auc(tpr_ada, fpr_ada)
    plt.plot(tpr_ada, fpr_ada, label=f'AdaBoost Class {i} (AUC = {roc_auc_ada:.2f})')

# Plot the diagonal line (no skill classifier)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='No skill')

# Customize the plot
plt.title('ROC Curve for Random Forest and AdaBoost', fontsize=14)
plt.xlabel('True Positive Rate (TPR)', fontsize=12)
plt.ylabel('False Positive Rate (FPR)', fontsize=12)
plt.legend(loc='lower right')
plt.grid(True)

# Show the plot
plt.show()

"""##Comparison between best models"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Data for the table
data = {
    "Model": [
        "KNN (Euclidean)", "KNN (Manhattan)", "KNN (Cosine)",
        "Logistic Regression (L1)", "Logistic Regression (L2)",
        "SVM (Linear)", "SVM (Poly)", "SVM (RBF)",
        "AdaBoost", "Random Forest"
    ],
    "Accuracy": [0.9737, 0.9561, 0.9474, 0.9474, 0.9825, 0.9912, 0.9649, 0.9737, 0.9561, 0.9211],
    "Precision": [0.9736, 0.9567, 0.9515, 0.9553, 0.9834, 0.9915, 0.9650, 0.9758, 0.9586, 0.9322],
    "Recall": [0.9737, 0.9561, 0.9474, 0.9474, 0.9825, 0.9912, 0.9649, 0.9737, 0.9561, 0.9211],
    "F1-Score": [0.9736, 0.9563, 0.9482, 0.9485, 0.9826, 0.9913, 0.9646, 0.9740, 0.9566, 0.9229],
    "ROC AUC": [0.9643, 0.9518, 0.9540, 0.9625, 0.9875, 0.9996, 0.9949, 0.9996, 0.9603, 0.9200],
}

# Convert to DataFrame
df = pd.DataFrame(data)

# Metrics to visualize
metrics = ["Accuracy", "Precision", "Recall", "F1-Score", "ROC AUC"]

# Plot configuration
bar_width = 0.15
x = np.arange(len(df["Model"]))
colors = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd"]

# Create subplots for each metric
fig, ax = plt.subplots(figsize=(16, 8))
for i, metric in enumerate(metrics):
    ax.bar(x + i * bar_width, df[metric], bar_width, label=metric, color=colors[i])

# Add labels and title
ax.set_xlabel("Model", fontsize=12)
ax.set_ylabel("Performance Metric", fontsize=12)
ax.set_title("Performance Metrics of Machine Learning Models", fontsize=16)
ax.set_xticks(x + bar_width * (len(metrics) - 1) / 2)
ax.set_xticklabels(df["Model"], rotation=45, ha="right", fontsize=10)
ax.legend()

# Show plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Define the model names
models = ['KNN', 'Logistic Regression', 'SVM', 'AdaBoost', 'Random Forest']

# Define the metrics for each model
precision_class_0 = [0.98, 1.00, 1.00, 0.99, 0.99]
precision_class_1 = [0.97, 0.94, 0.92, 0.89, 0.80]
recall_class_0 = [0.99, 0.97, 0.96, 0.95, 0.90]
recall_class_1 = [0.94, 1.00, 1.00, 0.97, 0.97]
accuracy = [0.97, 0.98, 0.97, 0.96, 0.92]
macro_precision = [0.97, 0.97, 0.96, 0.94, 0.90]
macro_recall = [0.96, 0.99, 0.98, 0.96, 0.94]

# Create subplots
fig, axs = plt.subplots(2, 2, figsize=(14, 12))

# Precision comparison for Class 0 and Class 1
axs[0, 0].bar(models, precision_class_0, label='Precision Class 0', alpha=0.7)
axs[0, 0].bar(models, precision_class_1, label='Precision Class 1', alpha=0.7)
axs[0, 0].set_title('Precision Comparison')
axs[0, 0].set_ylabel('Precision')
axs[0, 0].legend()

# Recall comparison for Class 0 and Class 1
axs[0, 1].bar(models, recall_class_0, label='Recall Class 0', alpha=0.7)
axs[0, 1].bar(models, recall_class_1, label='Recall Class 1', alpha=0.7)
axs[0, 1].set_title('Recall Comparison')
axs[0, 1].set_ylabel('Recall')
axs[0, 1].legend()

# Accuracy comparison
axs[1, 0].bar(models, accuracy, color='skyblue')
axs[1, 0].set_title('Accuracy Comparison')
axs[1, 0].set_ylabel('Accuracy')

# Macro Average Precision and Recall comparison
x = np.arange(len(models))
width = 0.35
axs[1, 1].bar(x - width/2, macro_precision, width, label='Macro Precision', color='orange')
axs[1, 1].bar(x + width/2, macro_recall, width, label='Macro Recall', color='green')
axs[1, 1].set_title('Macro Average Precision and Recall Comparison')
axs[1, 1].set_ylabel('Macro Average')
axs[1, 1].set_xticks(x)
axs[1, 1].set_xticklabels(models)
axs[1, 1].legend()

# Show the plot
plt.tight_layout()
plt.show()